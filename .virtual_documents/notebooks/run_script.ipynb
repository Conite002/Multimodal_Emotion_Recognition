import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
import torch.nn as nn
from pipelines.training.training_pipeline import train_model
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model


from pipelines.preprocessing.data_pipeline import generate_metadata
# tokenizer, model = load_text_model()
# embeddings = preprocess_text_for_model("Sample text", tokenizer, model)
# print(embeddings)





from pipelines.preprocessing.data_pipeline import generate_metadata
from preprocessing.audio.preprocess_audio import extract_audio
import os

video_dir = os.path.join("..", "data", "MELD.Raw", "train", "train_splits")
output_audio_dir = os.path.join("..", "data", "MELD.Raw", "train", "audio")
csv_file = os.path.join("..", "data", "MELD.Raw", "train", "train_sent_emo.csv")
output_json_path = os.path.join("..", "outputs", "preprocessed", "train_data.json")     # Path to save the preprocessed data    

train_audio_metadata = extract_audio(video_dir, output_audio_dir)



generate_metadata(csv_file, train_audio_metadata, output_json_path, dataset_type="train")



# rename the file to preprocess_audio.py
from preprocessing.audio.preprocess_audio import rename_files

video_dir = os.path.join("..", "data", "MELD.Raw", "test", "output_repeated_splits_test")
rename_files(video_dir)


# Test Dataset
video_dir = os.path.join("..", "data", "MELD.Raw", "test", "output_repeated_splits_test")
output_audio_dir = os.path.join("..", "data", "MELD.Raw", "test", "audio")
csv_file = os.path.join("..", "data", "MELD.Raw", "test_sent_emo.csv")
output_json_path = os.path.join("..", "outputs", "preprocessed", "test_data.json")
test_audio_metadata = extract_audio(video_dir, output_audio_dir)



generate_metadata(csv_file, test_audio_metadata, output_json_path, dataset_type="test")



# Dev Dataset
video_dir = os.path.join("..", "data", "MELD.Raw", "dev", "dev_splits_complete")
output_audio_dir = os.path.join("..", "data", "MELD.Raw", "dev", "audio")
csv_file = os.path.join("..", "data", "MELD.Raw", "dev_sent_emo.csv")
output_json_path = os.path.join("..", "outputs", "preprocessed", "dev_data.json")
dev_audio_metadata = extract_audio(video_dir, output_audio_dir)


generate_metadata(csv_file, dev_audio_metadata, output_json_path, dataset_type="dev")





from utils.dataloader import create_data_loaders

train_data_json_path = os.path.join("..", "outputs", "preprocessed", "train_data.json")
dev_data_json_path = os.path.join("..", "outputs", "preprocessed", "dev_data.json")
test_data_json_path = os.path.join("..", "outputs", "preprocessed", "test_data.json")

dims = {"audio": 100, "text": 768, "video": 512}
train_loaders, val_loaders, label_mapping = create_data_loaders(train_data_json_path, dev_data_json_path, dims, batch_size=32)
test_loaders, _, _ = create_data_loaders(test_data_json_path, test_data_json_path, dims, batch_size=32)


print("Label Mapping:", label_mapping)



# total length of the dataset
print(len(train_loaders["audio"].dataset))
# total length of labels
print(len(train_loaders["audio"].dataset.tensors[1]))
train_loaders["audio"].dataset.tensors[1]



train_loaders, val_loaders, test_loaders





# Save train, val, and test datasets
saving_dir = os.path.join("..", "outputs", "embeddings")
import torch

torch.save({
    'train': {
        'audio': train_loaders['audio'].dataset,
        'text': train_loaders['text'].dataset,
        'video': train_loaders['video'].dataset,
        'labels': train_loaders['audio'].dataset.tensors[1]
    },
    'val': {
        'audio': val_loaders['audio'].dataset,
        'text': val_loaders['text'].dataset,
        'video': val_loaders['video'].dataset,
        'labels': val_loaders['audio'].dataset.tensors[1]
    },
    'test': {
        'audio': test_loaders['audio'].dataset,
        'text': test_loaders['text'].dataset,
        'video': test_loaders['video'].dataset,
        'labels': test_loaders['audio'].dataset.tensors[1]
    }
}, os.path.join(saving_dir, "loaders_datasets.pt"))






import os
import torch
from torch.utils.data import DataLoader

saving_dir = os.path.join("..", "outputs", "embeddings")
saved_data = torch.load(os.path.join(saving_dir, "loaders_datasets.pt"))

train_loaders = {
    'audio': DataLoader(saved_data['train']['audio'], batch_size=32, shuffle=True),
    'text': DataLoader(saved_data['train']['text'], batch_size=32, shuffle=True),
    'video': DataLoader(saved_data['train']['video'], batch_size=32, shuffle=True),
    'label': DataLoader(saved_data['train']['labels'], batch_size=32, shuffle=True)
}

val_loaders = {
    'audio': DataLoader(saved_data['val']['audio'], batch_size=32, shuffle=False),
    'text': DataLoader(saved_data['val']['text'], batch_size=32, shuffle=False),
    'video': DataLoader(saved_data['val']['video'], batch_size=32, shuffle=False),
    'label': DataLoader(saved_data['val']['labels'], batch_size=32, shuffle=False)
}

test_loaders = {
    'audio': DataLoader(saved_data['test']['audio'], batch_size=32, shuffle=False),
    'text': DataLoader(saved_data['test']['text'], batch_size=32, shuffle=False),
    'video': DataLoader(saved_data['test']['video'], batch_size=32, shuffle=False),
    'label': DataLoader(saved_data['test']['labels'], batch_size=32, shuffle=False)
}










audio_model = AudioCNNClassifier(input_dim=768, num_classes=7)
train_model_audio = train_model(audio_model, train_loaders["audio"], val_loaders["audio"], num_epochs=500, learning_rate=1e-3, device="cpu", modal="audio", logfile=os.path.join("..", "logs", "training_logs", "audio_train.log"), verbose=False)



test_audio_loss, test_audio_acc, precision_audio, recall_audio, f1_audio = evaluate_model(train_model_audio, test_loaders["audio"], device="cpu", criterion=nn.CrossEntropyLoss(), modal="audio", logfile=os.path.join("..", "logs", "training_logs", "audio_test.log"))
print(f" Test Loss: {test_audio_loss}, Test Accuracy: {test_audio_acc}")
# Save the model
saving_dir = os.path.join("..", "outputs", "embeddings")
torch.save(train_model_audio.state_dict(), os.path.join('..', 'outputs', 'models',"audio_model.pt"))


from models.text.text_model import TextLSTMClassifier
                    
text_model = TextLSTMClassifier(input_dim=768, num_classes=7)

trained_text_model = train_model(
    text_model,
    train_loaders["text"],
    val_loaders["text"],
    num_epochs=500,
    learning_rate=1e-3,
    device="cpu",
    modal="text",
    logfile=os.path.join("..", "logs", "training_logs", "texte_train.log"),
    verbose=False
)




test_loss_text, test_accuracy_text, precision_text, recall_text, f1_text = evaluate_model(
    trained_text_model,
    test_loaders["text"],
    criterion=nn.CrossEntropyLoss(),
    device="cpu",
    modal="text",
    logfile=os.path.join("..", "logs", "training_logs", "text_test.log")
)

print(f"Text Test Loss: {test_loss_text:.4f}, Text Test Accuracy: {test_accuracy_text:.2f}% Text Precision: {precision_text:.2f}% Text Recall: {recall_text:.2f}% Text F1: {f1_text:.2f}%")
# Save the model
torch.save(trained_text_model.state_dict(), os.path.join('..', 'outputs', 'models',"text_model.pt"))





from models.video.video_model import VideoMLPClassifier

# Define the video model
video_model = VideoMLPClassifier(input_dim=768, num_classes=7)

# Train and validate the video model
trained_video_model = train_model(
    video_model,
    train_loaders["video"],
    val_loaders["video"],
    num_epochs=500,
    learning_rate=1e-3,
    device="cpu",
    modal="video",
    logfile=os.path.join("..", "logs", "training_logs", "video_train.log"),
    verbose=False
)



# Evaluate the video model on the test set
test_loss_video, test_accuracy_video, precision_video, recall_video, f1_video = evaluate_model(
    trained_video_model,
    test_loaders["video"],
    criterion=nn.CrossEntropyLoss(),
    device="cpu",
    modal="video",
    logfile=os.path.join("..", "logs", "training_logs", "video_test.log")
)
precision_video = precision_video * 100
recall_video = recall_video * 100
f1_video = f1_video * 100
print(f"Video Test Loss: {test_loss_video:.4f}, Video Test Accuracy: {test_accuracy_video:.2f}% Video Precision: {precision_video:.2f}% Video Recall: {recall_video:.2f}% Video F1: {f1_video:.2f}%")
# Save the model
torch.save(trained_video_model.state_dict(), os.path.join('..', 'outputs', 'models',"video_model.pt"))









import torch
import numpy as np

from models.bigru_coattention.multimodal import MultiModalDataset
saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)



# load the models
from models.audio.audio_model import AudioCNNClassifier
from models.text.text_model import TextLSTMClassifier
from models.video.video_model import VideoMLPClassifier

audio_model = AudioCNNClassifier(input_dim=768, num_classes=7)
audio_model.load_state_dict(torch.load(os.path.join('..', 'outputs', 'models',"audio_model.pt")))
audio_model.eval()

text_model = TextLSTMClassifier(input_dim=768, num_classes=7)
text_model.load_state_dict(torch.load(os.path.join('..', 'outputs', 'models',"text_model.pt")))
text_model.eval()

video_model = VideoMLPClassifier(input_dim=768, num_classes=7)
video_model.load_state_dict(torch.load(os.path.join('..', 'outputs', 'models',"video_model.pt")))
video_model.eval()



def majority_voting(audio_model, text_model, video_model, test_loader, device="cpu"):
    all_predictions = []
    all_labels = []

    audio_model.eval()
    text_model.eval()
    video_model.eval()

    with torch.no_grad():
        for batch in test_loader:
            audio_data, text_data, video_data, labels = batch

            # Move data to device
            audio_data = audio_data.to(device)
            text_data = text_data.to(device)
            video_data = video_data.to(device)
           
            audio_data = audio_data.unsqueeze(1)
            text_data = text_data.unsqueeze(1)




            audio_preds = audio_model(audio_data).argmax(dim=1).cpu().numpy()
            text_preds = text_model(text_data).argmax(dim=1).cpu().numpy()
            video_preds = video_model(video_data).argmax(dim=1).cpu().numpy()


            # Majority vote
            stacked_preds = np.stack([audio_preds, text_preds, video_preds], axis=0)
            final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=stacked_preds)

            all_predictions.extend(final_preds)
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    from sklearn.metrics import accuracy_score, precision_recall_fscore_support

    accuracy = accuracy_score(all_labels, all_predictions)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average="weighted")

    return accuracy, precision, recall, f1




accuracy, precision, recall, f1 = majority_voting(audio_model, text_model, video_model, test_loader)
print(f"Voting Accuracy: {accuracy * 100:.2f}%, Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1 * 100:.2f}%")









import os
import torch
from torch.utils.data import DataLoader
import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
from pipelines.preprocessing.data_pipeline import generate_metadata
import numpy as np
import torch.nn as nn
from pipelines.training.training_pipeline import train_model, train_model_coattention
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model

from torch import nn, optim
from tqdm import tqdm
from models.bigru_coattention.coattention import CoAttentionFusion
from utils.logger import create_logger
from sklearn.metrics import precision_score, recall_score, f1_score
from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention
from models.bigru_coattention.multimodal import MultiModalDataset





import torch
import numpy as np

saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# --------------------------------------------------------------------------------
# Train Model
# --------------------------------------------------------------------------------

co_attention_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=7 )
trained_model_coattention = train_model_coattention(co_attention_model, train_loader, val_loader, num_epochs=500, learning_rate=1e-3, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "coattention_train.log"), verbose=False)
#  save the model




# load the model
co_attention_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=7 )
co_attention_model.load_state_dict(torch.load(os.path.join('..', 'outputs', 'models','bigru_coattention',"coattention_model.pt")))
co_attention_model.eval()



test_accuracy, precision, recall, f1 = test_model_coattention(trained_model_coattention, test_loader, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "coattention_test.log"))



# save the model
torch.save(trained_model_coattention.state_dict(), os.path.join('..', 'outputs', 'models','bigru_coattention',"coattention_model.pt"))
# load the model
# co_attention_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=7 )
# co_attention_model.load_state_dict(torch.load(os.path.join('..', 'outputs', 'models','bigru_coattention',"coattention_model.pt")))
# co_attention_model.eval()


import os
import torch
from torch.utils.data import DataLoader
import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
from pipelines.preprocessing.data_pipeline import generate_metadata
import numpy as np
import torch.nn as nn
from pipelines.training.training_pipeline import train_model, train_model_coattention
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model

from torch import nn, optim
from tqdm import tqdm
from models.bigru_coattention.coattention import CoAttentionFusion
from utils.logger import create_logger
from sklearn.metrics import precision_score, recall_score, f1_score
from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention
from models.bigru_coattention.multimodal import MultiModalDataset





import torch
import numpy as np

saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# --------------------------------------------------------------------------------
# Train Model
# --------------------------------------------------------------------------------

co_attention_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=7 )
trained_model_coattention = train_model_coattention(co_attention_model, train_loader, val_loader, num_epochs=50, learning_rate=1e-3, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "coattention_train.log"), verbose=False)
#  save the model




co_attention_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=7 )
co_attention_model.load_state_dict(torch.load(os.path.join("best_model_bigru_coatention.pth")))
co_attention_model.eval()


test_accuracy, precision, recall, f1 = test_model_coattention(trained_model_coattention, test_loader, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "coattention_test.log"))


test_accuracy, precision, recall, f1 = test_model_coattention(co_attention_model, test_loader, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "coattention_test.log"))


import os
import torch
from torch.utils.data import DataLoader
import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
from pipelines.preprocessing.data_pipeline import generate_metadata
import numpy as np
import torch.nn as nn
from pipelines.training.training_pipeline import train_model, train_model_coattention
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model

from torch import nn, optim
from tqdm import tqdm
from models.bigru_coattention.coattention import CoAttentionFusion, CoAttentionFusionReguNorm
from utils.logger import create_logger
from sklearn.metrics import precision_score, recall_score, f1_score
from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention
from models.bigru_coattention.multimodal import MultiModalDataset





import torch
import numpy as np

saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets_reduced_label_dim_4.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# --------------------------------------------------------------------------------
# Train Model
# --------------------------------------------------------------------------------
num_classes = 4
co_attentionReguNorm_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=num_classes )
trained_model_coattention_regu_norm = train_model_coattention(co_attentionReguNorm_model, train_loader, val_loader, num_classes=num_classes, num_epochs=50, learning_rate=1e-4, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "bigru_coattention_train.log"), verbose=False, save_model='bigru_coattention_4.pth')
#  save the model




test_accuracy, precision, recall, f1 = test_model_coattention(trained_model_coattention_regu_norm, test_loader, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "bigru_coattention_4_test.log"), num_classes=num_classes)





import os
import torch
from torch.utils.data import DataLoader
import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
from pipelines.preprocessing.data_pipeline import generate_metadata
import numpy as np
import torch.nn as nn
from pipelines.training.training_pipeline import train_model, train_model_coattention
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model

from torch import nn, optim
from tqdm import tqdm
from models.bigru_coattention.coattention import CoAttentionFusion, CoAttentionFusionReguNorm
from utils.logger import create_logger
from sklearn.metrics import precision_score, recall_score, f1_score
from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention
from models.bigru_coattention.multimodal import MultiModalDataset





import torch
import numpy as np

saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# --------------------------------------------------------------------------------
# Train Model
# --------------------------------------------------------------------------------
num_classes = 7
co_attentionReguNorm_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=num_classes )
trained_model_coattention_regu_norm = train_model_coattention(co_attentionReguNorm_model, train_loader, val_loader, num_classes=num_classes, num_epochs=50, learning_rate=1e-4, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "bigru_coattention_train.log"), verbose=False, save_model='bigru_coattention_4.pth')
#  save the model




test_accuracy, precision, recall, f1 = test_model_coattention(trained_model_coattention_regu_norm, test_loader, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "bigru_coattention_16_head_test.log"), num_classes=num_classes)





import os
import torch
from torch.utils.data import DataLoader
import sys, os
sys.path.append(os.path.abspath('..'))
from preprocessing.text.preprocess_text import preprocess_text_for_model, load_text_model
from preprocessing.audio.preprocess_audio import preprocess_audio_for_model, load_audio_model, extract_audio
from pipelines.preprocessing.data_pipeline import generate_metadata
import numpy as np
import torch.nn as nn
from pipelines.training.training_pipeline import train_model, train_model_coattention
from models.audio.audio_model import AudioCNNClassifier
from pipelines.training.training_pipeline import evaluate_model

from torch import nn, optim
from tqdm import tqdm
from models.bigru_coattention.coattention import CoAttentionFusion, CoAttentionFusionReguNorm
from utils.logger import create_logger
from sklearn.metrics import precision_score, recall_score, f1_score
from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention
from models.bigru_coattention.multimodal import MultiModalDataset





import torch
import numpy as np

saved_data = torch.load(os.path.join("..", "outputs", "embeddings", "loaders_datasets.pt"))


train_dataset = MultiModalDataset(
    saved_data['train']['audio'],
    saved_data['train']['text'],
    saved_data['train']['video'],
    saved_data['train']['labels']
)

val_dataset = MultiModalDataset(
    saved_data['val']['audio'],
    saved_data['val']['text'],
    saved_data['val']['video'],
    saved_data['val']['labels']
)

test_dataset = MultiModalDataset(
    saved_data['test']['audio'],
    saved_data['test']['text'],
    saved_data['test']['video'],
    saved_data['test']['labels']
)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)



# --------------------------------------------------------------------------------
# Train Model
# --------------------------------------------------------------------------------
num_classes = 7
co_attentionReguNorm_model = CoAttentionFusion(input_dim_audio=768, input_dim_text=768, input_dim_video=768, num_classes=num_classes )
trained_model_coattention_regu_norm = train_model_coattention(co_attentionReguNorm_model, train_loader, val_loader, num_classes=num_classes, num_epochs=50, learning_rate=1e-4, device="cpu", logfile=os.path.join("..", "logs", "training_logs", "bigru_coattention_train.log"), verbose=False, save_model='bigru_coattention_4.pth')
#  save the model


















import pandas as pd
import os
from utils.speaker import add_speaker_to_json

train_json_path = os.path.join("..", "outputs", "preprocessed", "train_data.json")
test_json_path = os.path.join("..", "outputs", "preprocessed", "test_data.json")
dev_json_path = os.path.join("..", "outputs", "preprocessed", "dev_data.json")

csv_file_train = os.path.join("..", "data", "MELD.Raw", "train", "train_sent_emo.csv")
csv_file_test = os.path.join("..", "data", "MELD.Raw", "test_sent_emo.csv")
csv_file_dev = os.path.join("..", "data", "MELD.Raw", "dev_sent_emo.csv")

train_df = pd.read_csv(csv_file_train)
test_df = pd.read_csv(csv_file_test)
dev_df = pd.read_csv(csv_file_dev)

add_speaker_to_json(train_json_path, train_df)
add_speaker_to_json(test_json_path, test_df)
add_speaker_to_json(dev_json_path, dev_df)



import torch
from torch.utils.data import DataLoader

import torch
import torch.nn as nn
import os, json, sys
from models.fusion_modals.transformerMultimodal import TransformerMultimodal, validate_model_TF_FUSION_MULTIMODAL, test_model_TF_FUSION_MULTIMODAL, train_model_TF_FUSION_MULTIMODAL
import torch.nn as nn
from models.bigru_coattention.multimodal import MultiModalDataset, MultiModalDatasetWithSpeaker
from utils.dataloader import extract_tensors_from_tensordataset


saving_dir = os.path.join("..", "outputs", "embeddings")
saved_data = torch.load(os.path.join(saving_dir, "loaders_datasets.pt"))

train_audio = extract_tensors_from_tensordataset(saved_data['train']['audio'])
train_text = extract_tensors_from_tensordataset(saved_data['train']['text'])
train_video = extract_tensors_from_tensordataset(saved_data['train']['video'])

val_audio = extract_tensors_from_tensordataset(saved_data['val']['audio'])
val_text = extract_tensors_from_tensordataset(saved_data['val']['text'])
val_video = extract_tensors_from_tensordataset(saved_data['val']['video'])

test_audio = extract_tensors_from_tensordataset(saved_data['test']['audio'])
test_text = extract_tensors_from_tensordataset(saved_data['test']['text'])
test_video = extract_tensors_from_tensordataset(saved_data['test']['video'])



# File paths
train_json_path = os.path.join("..", "outputs", "preprocessed", "train_data.json")
val_json_path = os.path.join("..", "outputs", "preprocessed", "dev_data.json")
test_json_path = os.path.join("..", "outputs", "preprocessed", "test_data.json")
train_dataset = MultiModalDatasetWithSpeaker(
    audio_data=train_audio,
    text_data=train_text,
    video_data=train_video,
    labels=saved_data['train']['labels'],
    json_path=train_json_path
)

val_dataset = MultiModalDatasetWithSpeaker(
    audio_data=val_audio,
    text_data=val_text,
    video_data=val_video,
    labels=saved_data['val']['labels'],
    json_path=val_json_path,
    speaker_to_id=train_dataset.speaker_to_id 
)

test_dataset = MultiModalDatasetWithSpeaker(
    audio_data=test_audio,
    text_data=test_text,
    video_data=test_video,
    labels=saved_data['test']['labels'],
    json_path=test_json_path,
    speaker_to_id=train_dataset.speaker_to_id 
)



train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
# --------------------------------------------------------------------------------
# Workflow for Training
# --------------------------------------------------------------------------------
# Model parameters
num_classes = 7
num_speakers = len(train_dataset.speaker_to_id)
hidden_dim = 128

# Instantiate the model
model_TransformerMultimodal = TransformerMultimodal(
    text_dim=768,
    audio_dim=768,
    video_dim=768,
    hidden_dim=hidden_dim,
    num_classes=num_classes,
    num_speakers=num_speakers,
    speaker_dim=64
)


from tqdm import tqdm
trained_model_TF_F_M = train_model_TF_FUSION_MULTIMODAL(
    model_TransformerMultimodal,
    train_loader,
    val_loader,
    num_epochs=200,
    learning_rate=1e-3,
    device="cpu" if torch.cuda.is_available() else "cpu",
    logfile=os.path.join("..", "logs", "training_logs", "transformer_multimodal_train.log"),
    verbose=False
)


test_accuracy, all_labels, all_predictions,  = test_model_TF_FUSION_MULTIMODAL(trained_model_TF_F_M, test_loader, device="cpu" if torch.cuda.is_available() else "cpu")
print(f"Final Test Accuracy: {test_accuracy:.2f}%")


from sklearn.metrics import classification_report

# Generate the classification report
print("Classification Report:")
print(classification_report(all_labels, all_predictions, target_names=[f"Class {i}" for i in range(num_classes)]))









saving_dir = os.path.join("..", "outputs", "embeddings")
saved_data = torch.load(os.path.join(saving_dir, "loaders_datasets.pt"))


train_audio = extract_tensors_from_tensordataset(saved_data['train']['audio'])
train_text = extract_tensors_from_tensordataset(saved_data['train']['text'])
train_video = extract_tensors_from_tensordataset(saved_data['train']['video'])

val_audio = extract_tensors_from_tensordataset(saved_data['val']['audio'])
val_text = extract_tensors_from_tensordataset(saved_data['val']['text'])
val_video = extract_tensors_from_tensordataset(saved_data['val']['video'])

test_audio = extract_tensors_from_tensordataset(saved_data['test']['audio'])
test_text = extract_tensors_from_tensordataset(saved_data['test']['text'])
test_video = extract_tensors_from_tensordataset(saved_data['test']['video'])



# File paths
train_json_path = os.path.join("..", "outputs", "preprocessed", "train_data.json")
val_json_path = os.path.join("..", "outputs", "preprocessed", "dev_data.json")
test_json_path = os.path.join("..", "outputs", "preprocessed", "test_data.json")
train_dataset = MultiModalDatasetWithSpeaker(
    audio_data=train_audio,
    text_data=train_text,
    video_data=train_video,
    labels=saved_data['train']['labels'],
    json_path=train_json_path
)

val_dataset = MultiModalDatasetWithSpeaker(
    audio_data=val_audio,
    text_data=val_text,
    video_data=val_video,
    labels=saved_data['val']['labels'],
    json_path=val_json_path,
    speaker_to_id=train_dataset.speaker_to_id 
)

test_dataset = MultiModalDatasetWithSpeaker(
    audio_data=test_audio,
    text_data=test_text,
    video_data=test_video,
    labels=saved_data['test']['labels'],
    json_path=test_json_path,
    speaker_to_id=train_dataset.speaker_to_id 
)



train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
