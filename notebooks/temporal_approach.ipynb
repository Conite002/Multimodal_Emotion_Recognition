{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pipelines.training.training_pipeline import train_model, train_model_coattention\n",
    "from models.audio.audio_model import AudioCNNClassifier\n",
    "from pipelines.training.training_pipeline import evaluate_model\n",
    "\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from models.bigru_coattention.coattention import CoAttentionFusion\n",
    "from utils.logger import create_logger\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from pipelines.evaluation.evaluation_pipeline import test_model_coattention, evaluate_model_coattention\n",
    "from models.bigru_coattention.multimodal import MultiModalDataset\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_data = torch.load(os.path.join(\"..\", \"outputs\", \"embeddings\", \"loaders_datasets.pt\"))\n",
    "\n",
    "train_dataset = MultiModalDataset(\n",
    "    saved_data['train']['audio'],\n",
    "    saved_data['train']['text'],\n",
    "    saved_data['train']['video'],\n",
    "    saved_data['train']['labels']\n",
    ")\n",
    "\n",
    "val_dataset = MultiModalDataset(\n",
    "    saved_data['val']['audio'],\n",
    "    saved_data['val']['text'],\n",
    "    saved_data['val']['video'],\n",
    "    saved_data['val']['labels']\n",
    ")\n",
    "\n",
    "test_dataset = MultiModalDataset(\n",
    "    saved_data['test']['audio'],\n",
    "    saved_data['test']['text'],\n",
    "    saved_data['test']['video'],\n",
    "    saved_data['test']['labels']\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_windows(data, labels, window_size=5):\n",
    "    \"\"\"\n",
    "    Crée des fenêtres temporelles autour de chaque sample.\n",
    "    :param data: Tensor de taille (num_samples, feature_dim)\n",
    "    :param labels: Tensor de labels associés (num_samples,)\n",
    "    :param window_size: Taille de la fenêtre temporelle\n",
    "    :return: data_windows, labels\n",
    "    \"\"\"\n",
    "    half_window = window_size // 2\n",
    "    num_samples, feature_dim = data.size()\n",
    "    \n",
    "    padded_data = torch.cat([\n",
    "        torch.zeros((half_window, feature_dim)),\n",
    "        data,\n",
    "        torch.zeros((half_window, feature_dim)) \n",
    "    ], dim=0)\n",
    "    \n",
    "    data_windows = []\n",
    "    for i in range(num_samples):\n",
    "        window = padded_data[i:i+window_size]\n",
    "        data_windows.append(window)\n",
    "    \n",
    "    return torch.stack(data_windows), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TemporalAttentionModel(nn.Module):\n",
    "    def __init__(self, input_dim_audio, input_dim_video, input_dim_text, hidden_dim, num_heads=4, num_layers=2, dropout_rate=0.3):\n",
    "        super(TemporalAttentionModel, self).__init__()\n",
    "\n",
    "        self.audio_projection = nn.Linear(input_dim_audio, 100)  \n",
    "        self.video_projection = nn.Linear(input_dim_video, 512)\n",
    "        \n",
    "        self.audio_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=100, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout_rate),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.video_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=512, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout_rate),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.text_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=768, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout_rate),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.fusion = nn.Linear(100 + 512 + 768, hidden_dim)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 7)  \n",
    "        )\n",
    "\n",
    "    def forward(self, audio, video, text):\n",
    "        audio = audio.to(self.audio_projection.weight.device)  \n",
    "        video = video.to(self.video_projection.weight.device)\n",
    "        text = text.to(self.text_transformer.layers[0].linear1.weight.device) \n",
    "        \n",
    "        audio = self.audio_projection(audio)\n",
    "        video = self.video_projection(video)\n",
    "        audio = self.audio_transformer(audio.permute(1, 0, 2))\n",
    "        video = self.video_transformer(video.permute(1, 0, 2))\n",
    "        text = self.text_transformer(text.permute(1, 0, 2))\n",
    "\n",
    "        audio = audio.mean(dim=0)\n",
    "        video = video.mean(dim=0)\n",
    "        text = text.mean(dim=0)\n",
    "\n",
    "        combined = torch.cat([audio, video, text], dim=-1)\n",
    "        combined = self.fusion(combined)\n",
    "\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Module pour appliquer une attention pondérée sur la dimension temporelle.\n",
    "        Args:\n",
    "            input_dim (int): Taille des features (embedding size).\n",
    "        \"\"\"\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.query = nn.Linear(input_dim, input_dim) \n",
    "        self.key = nn.Linear(input_dim, input_dim)    \n",
    "        self.value = nn.Linear(input_dim, input_dim)  \n",
    "        self.scale = input_dim ** 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor de taille (batch_size, seq_length, feature_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor réduit avec attention (batch_size, feature_dim)\n",
    "        \"\"\"\n",
    "        Q = self.query(x) \n",
    "        K = self.key(x)   \n",
    "        V = self.value(x) \n",
    "\n",
    "        # Calcul de l'attention : scores = softmax(Q * K^T / sqrt(d))\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale  \n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        context = torch.matmul(attn_weights, V)\n",
    "\n",
    "        context = context.mean(dim=1) \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(VideoAttention, self).__init__()\n",
    "        self.temporal_attention = TemporalAttention(input_dim)\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Vidéo Tensor de taille (batch_size, seq_length, feature_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor réduit avec attention (batch_size, hidden_dim)\n",
    "        \"\"\"\n",
    "        context = self.temporal_attention(x)\n",
    "        context = self.fc(context) \n",
    "        return context\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class MultiModalTemporalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio, video, text, labels):\n",
    "        self.audio = audio\n",
    "        self.video = video\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.audio[idx], self.video[idx], self.text[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train video data shape after reduction: torch.Size([9988, 512])\n",
      "Validation video data shape after reduction: torch.Size([1108, 512])\n",
      "Test video data shape after reduction: torch.Size([2610, 512])\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 313/313 [8:18:30<00:00, 95.56s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 500.4262, Val Loss: 1.6993, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 313/313 [6:18:02<00:00, 72.47s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Train Loss: 490.0890, Val Loss: 1.6578, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 313/313 [5:20:34<00:00, 61.45s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Train Loss: 491.4391, Val Loss: 1.6269, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 313/313 [5:23:39<00:00, 62.04s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Train Loss: 488.7635, Val Loss: 1.6293, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 313/313 [5:26:00<00:00, 62.49s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Train Loss: 485.8740, Val Loss: 1.6265, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 313/313 [5:25:27<00:00, 62.39s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Train Loss: 484.3572, Val Loss: 1.6556, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 313/313 [5:25:53<00:00, 62.47s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Train Loss: 484.4915, Val Loss: 1.6195, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 313/313 [5:26:04<00:00, 62.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Train Loss: 480.2535, Val Loss: 1.6331, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 313/313 [5:25:25<00:00, 62.38s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Train Loss: 477.9623, Val Loss: 1.6020, Val Accuracy: 42.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 313/313 [6:31:58<00:00, 75.14s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Train Loss: 473.0247, Val Loss: 1.5959, Val Accuracy: 42.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 313/313 [5:23:47<00:00, 62.07s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Train Loss: 471.6221, Val Loss: 1.5790, Val Accuracy: 42.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 313/313 [5:24:05<00:00, 62.13s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Train Loss: 469.4962, Val Loss: 1.5764, Val Accuracy: 42.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 313/313 [5:24:14<00:00, 62.16s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Train Loss: 468.4104, Val Loss: 1.5775, Val Accuracy: 41.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 313/313 [5:23:11<00:00, 61.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Train Loss: 467.6366, Val Loss: 1.5929, Val Accuracy: 42.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15:  68%|██████▊   | 214/313 [3:59:17<1:50:42, 67.09s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20463616 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 321\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader), accuracy\n\u001b[0;32m    319\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 321\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m           \u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 287\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[0;32m    285\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)  \n\u001b[0;32m    286\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m--> 287\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    289\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\mer-gpu\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\mer-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20463616 bytes."
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "video_attention_module = VideoAttention(input_dim=768, hidden_dim=512)\n",
    "video_attention_module = video_attention_module.to(device) \n",
    "\n",
    "reduced_video_data_train = torch.stack([\n",
    "    video_attention_module(video_sample.unsqueeze(0).to(device)).squeeze(0).cpu()\n",
    "    for video_sample in train_dataset.video_data\n",
    "])\n",
    "\n",
    "# Preprocess validation dataset\n",
    "reduced_video_data_val = torch.stack([\n",
    "    video_attention_module(video_sample.unsqueeze(0).to(device)).squeeze(0).cpu()\n",
    "    for video_sample in val_dataset.video_data\n",
    "])\n",
    "\n",
    "# Preprocess test dataset\n",
    "reduced_video_data_test = torch.stack([\n",
    "    video_attention_module(video_sample.unsqueeze(0).to(device)).squeeze(0).cpu()\n",
    "    for video_sample in test_dataset.video_data\n",
    "])\n",
    "\n",
    "train_dataset.video_data = reduced_video_data_train\n",
    "print(\"Train video data shape after reduction:\", train_dataset.video_data.shape)\n",
    "val_dataset.video_data = reduced_video_data_val\n",
    "print(\"Validation video data shape after reduction:\", val_dataset.video_data.shape)  \n",
    "\n",
    "test_dataset.video_data = reduced_video_data_test\n",
    "print(\"Test video data shape after reduction:\", test_dataset.video_data.shape)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------\n",
    "window_size = 5\n",
    "\n",
    "# Create temporal windows for training dataset\n",
    "audio_windows_train, labels_train = create_temporal_windows(train_dataset.audio_data, train_dataset.labels, window_size=window_size)\n",
    "video_windows_train, _ = create_temporal_windows(train_dataset.video_data, train_dataset.labels, window_size=window_size)\n",
    "text_windows_train, _ = create_temporal_windows(train_dataset.text_data, train_dataset.labels, window_size=window_size)\n",
    "\n",
    "# Create temporal windows for validation dataset\n",
    "audio_windows_val, labels_val = create_temporal_windows(val_dataset.audio_data, val_dataset.labels, window_size=window_size)\n",
    "video_windows_val, _ = create_temporal_windows(val_dataset.video_data, val_dataset.labels, window_size=window_size)\n",
    "text_windows_val, _ = create_temporal_windows(val_dataset.text_data, val_dataset.labels, window_size=window_size)\n",
    "\n",
    "# Create temporal windows for test dataset\n",
    "audio_windows_test, labels_test = create_temporal_windows(test_dataset.audio_data, test_dataset.labels, window_size=window_size)\n",
    "video_windows_test, _ = create_temporal_windows(test_dataset.video_data, test_dataset.labels, window_size=window_size)\n",
    "text_windows_test, _ = create_temporal_windows(test_dataset.text_data, test_dataset.labels, window_size=window_size)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "# Training Dataset and DataLoader\n",
    "train_dataset = MultiModalTemporalDataset(audio_windows_train, video_windows_train, text_windows_train, labels_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Validation Dataset and DataLoader\n",
    "val_dataset = MultiModalTemporalDataset(audio_windows_val, video_windows_val, text_windows_val, labels_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Test Dataset and DataLoader\n",
    "test_dataset = MultiModalTemporalDataset(audio_windows_test, video_windows_test, text_windows_test, labels_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "model = TemporalAttentionModel(\n",
    "    input_dim_audio=768,  \n",
    "    input_dim_video=512, \n",
    "    input_dim_text=768,  \n",
    "    hidden_dim=512,       \n",
    "    num_heads=4,          \n",
    "    num_layers=2,         \n",
    "    dropout_rate=0.3     \n",
    ")\n",
    "def train_model(model, train_loader, val_loader, num_epochs, learning_rate, device): \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Optimizer\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for idx, (audio, video, text, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")):\n",
    "            audio, video, text, labels = audio.to(device), video.to(device), text.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(audio, video, text)  \n",
    "            loss = criterion(outputs, labels)  \n",
    "            outputs = outputs.detach()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for audio, video, text, labels in dataloader:\n",
    "            audio, video, text, labels = audio.to(device), video.to(device), text.to(device), labels.to(device)\n",
    "            outputs = model(audio, video, text)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return total_loss / len(dataloader), accuracy\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=20,          \n",
    "    learning_rate=1e-3,     \n",
    "    device=device           \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mer-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
